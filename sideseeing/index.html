<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SideSeeing Project</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
    <script src="https://code.jquery.com/jquery-1.11.3.min.js"></script>
    <script src="js/utils.js"></script>
</head>

<body class="is-family-code" style="height: 100vh;">
    <nav class="navbar is-light is-spaced" style="border-bottom: 1px solid lightgray;" role="navigation" aria-label="main navigation">
        <div class="navbar-brand">
          <a class="navbar-item" href="">
            <strong>SideSeeing Project</strong>
          </a>
      
          <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false" data-target="navbarSP">
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
          </a>
        </div>
      
        <div id="navbarSP" class="navbar-menu">
          <div class="navbar-start">
            <a class="navbar-item" href="">
              Home
            </a>
      
            <a class="navbar-item" href="dataset.html">
              Dataset
            </a>
          </div>
        </div>
    </nav>
    
    <section class="section">
        <div class="container">
            <div id="home" class="content">
                <header class="content">
                    <h1>SideSeeing Project</h1>
                </header>
            
                <p><img src="img/sidewalks.png"></p>
            
                <div class="tile is-ancestor">
                    <div class="tile is-vertical is-parent">
                        <p>The <strong>SideSeeing Project</strong> aims to develop methods based on Computer Vision and Machine Learning for Urban Informatics applications. Our goal is to devise strategies for obtaining and analyzing data related to urban accessibility. The project is expected to consist of six modules:
                        </p>

                        <ol>
                            <li>Collection and generation of multimodal datasets;</li>
                            <li>Preprocessing;</li>
                            <li>Labeling;</li>
                            <li>Visualization;</li>
                            <li>Application of artificial intelligence tasks; and</li>
                            <li>Analysis of information for decision-making</li>    
                        </ol>

                        <div>
                            <img src="img/wearable.jpeg" width="200px" style="margin-right:12px; float:inline-start;border: 1px solid gray;">
                            <p>As part of the initial module, an application for a mobile device's operating system</li> is currently in the testing phase and is available <a href="app.html">here</a>.</p>
                            
                            <p>This application is capable of generating multimodal datasets using video cameras and sensors such as accelerometers, gyroscopes, and magnetometers from mobile phones attached to a low-cost wearable support.</p>
                        </div>
                    </div>

                    <div class="tile is-vertical is-parent">
                        <p>Our current efforts are focused on collecting data in regions of Brazil and the United States of America. An overview of what has already been collected can be viewed <a href="dataset.html">here</a>.
                        </p>

                        <p>This is a project about Urban Informatics, which is an area characterized by the development of models to represent cities. In this context, the intersection between digital technologies and available infrastructure plays a significant role. Data from sensors installed in the city's infrastructure enable the generation of models capable of providing inputs for decision-making. Examples of these inputs include characterizations of building facades, sidewalk conditions, presence of vegetation, region-specific noise levels, land use, among others.</p>

                        <p>These pieces of information can be in various data representations such as visual data captured by video cameras, sound data captured by microphones, textual data extracted from databases related to regions, and temporal data obtained from sensors in mobile devices, for instance. It is worth noting that real-world situations, especially those related to Urban Informatics, are inherently multimodal.</p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container">
            <div id="app" class="content">
                <header class="subtitle">The app MultiSensor Data Collection</header>
                <div>
                    <img src="img/icon.png" width="75px" style="margin-right:12px; float:inline-start;">
                    <p>MultiSensor Data Collection is a mobile app that seamlessly gathers data from various sensors, including accelerometer, gyroscope, magnetometer, GPS, video camera, and audio, allowing you to create a comprehensive dataset locally and effortlessly transmit it to the cloud for further analysis and storage. The applications' source-code is publicly available <a target="_blank" href="https://github.com/rafaelpezzuto/multi-sensor-data-collection">here</a>.</p>
                </div>
            </div>
        </div>
    </section>

    <footer class="footer" style="margin-top: 50px; border-top: 1px solid lightgrey;">
        <div class="content has-text-centered">
            <p>
                SideSeeing Project - 2024<br/>
                University of SÃ£o Paulo
            </p>
        </div>
    </footer>

</body>
</html>